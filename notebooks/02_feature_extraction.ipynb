{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to run this first!\n",
    "from utils_notebook_init import init_notebook\n",
    "init_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fccde6",
   "metadata": {},
   "source": [
    "Section 1: File Path Collection\n",
    "\n",
    "Collect all .wav file paths from the dataset folder (recursively) and infer their labels from filenames.\n",
    "Then we export the list to data/audio_filepaths.csv for reproducible access in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cf401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.data_loader import get_audio_file_paths, export_file_list_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/raw\"\n",
    "files, labels = get_audio_file_paths(data_dir)\n",
    "\n",
    "print(f\"✅ Found {len(files)} audio files in {data_dir}\")\n",
    "print(\"Example file paths:\", files[:3])\n",
    "print(\"Example labels:\", labels[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ab917",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = \"data/audio_filepaths.csv\"\n",
    "export_file_list_to_csv(files, labels, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_csv)\n",
    "print(f\"✅ CSV successfully created at: {output_csv}\")\n",
    "display(df.head())\n",
    "print(f\"Unique labels found: {df['label'].nunique()}\")\n",
    "print(\"Label distribution:\")\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a1761a",
   "metadata": {},
   "source": [
    "Section 2: Data Splitting\n",
    "\n",
    "Split the dataset into train, validation, and test sets using the file list from audio_filepaths.csv.\n",
    "Each subset keeps balanced class proportions (stratified).\n",
    "Output files:\n",
    "- data/train_split.csv\n",
    "- data/val_split.csv\n",
    "- data/test_split.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f38ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d570e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the audio file paths CSV\n",
    "df = pd.read_csv(\"data/audio_filepaths.csv\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split into train / test first\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55984cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split train into train / val\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"label\"]\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10afedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: save them for future reuse\n",
    "train_df.to_csv(\"data/train_split.csv\", index=False)\n",
    "val_df.to_csv(\"data/val_split.csv\", index=False)\n",
    "test_df.to_csv(\"data/test_split.csv\", index=False)\n",
    "print(\"✅ Saved data splits to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87baf9de",
   "metadata": {},
   "source": [
    "Section 3: Normalization\n",
    "\n",
    "Normalize audio waveforms to ensure consistent amplitude range.\n",
    "The mean and std are computed from the training set only,\n",
    "then applied to all splits for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8354f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "# --- Load split files ---\n",
    "train_df = pd.read_csv(\"data/train_split.csv\")\n",
    "val_df   = pd.read_csv(\"data/val_split.csv\")\n",
    "test_df  = pd.read_csv(\"data/test_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Step 1: Compute mean and std from training set ---\n",
    "def compute_mean_std(file_list):\n",
    "    all_samples = []\n",
    "    for f in file_list:\n",
    "        y, _ = librosa.load(f, sr=None, mono=True)\n",
    "        all_samples.append(y)\n",
    "    all_concat = np.concatenate(all_samples)\n",
    "    mean = np.mean(all_concat)\n",
    "    std = np.std(all_concat)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = compute_mean_std(train_df[\"filepath\"])\n",
    "print(f\"Training data mean: {mean:.6f}, std: {std:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77737163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Define normalization function ---\n",
    "def normalize_audio(file_path, mean, std):\n",
    "    y, sr = librosa.load(file_path, sr=None, mono=True)\n",
    "    # standardization\n",
    "    y_norm = (y - mean) / std   \n",
    "    return y_norm, sr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test on one file\n",
    "sample_file = train_df[\"filepath\"].iloc[0]\n",
    "y_norm, sr = normalize_audio(sample_file, mean, std)\n",
    "print(f\"✅ Normalized one file: {sample_file}, sr={sr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7774e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set mean=-0.000263, std=0.121323\n",
      "✅ Normalized one file successfully, shape=(112000,)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Test stabilized normalization function ---\n",
    "\n",
    "from src.preprocessing import fit_audio_normalizer, apply_audio_normalization\n",
    "\n",
    "# Compute global mean/std from training set\n",
    "mean, std = fit_audio_normalizer(train_df[\"filepath\"])\n",
    "print(f\"Training set mean={mean:.6f}, std={std:.6f}\")\n",
    "\n",
    "# Apply normalization on one example file\n",
    "y_norm, sr = apply_audio_normalization(train_df[\"filepath\"].iloc[0], mean, std)\n",
    "print(f\"✅ Normalized one file successfully, shape={y_norm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dccc3477-3742-416d-b66c-86872971bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Normalized 347 train, 62 val, 103 test files.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Apply Normalization to All Splits ---\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import soundfile as sf\n",
    "from src.preprocessing import apply_audio_normalization\n",
    "\n",
    "# Target directory to save normalized audio\n",
    "output_dir = \"data/normalized_wav\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def normalize_and_save(file_list, mean, std, output_dir):\n",
    "    normalized_paths = []\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for f in file_list:\n",
    "        y_norm, sr = apply_audio_normalization(f, mean, std)\n",
    "        out_path = os.path.join(output_dir, os.path.basename(f))\n",
    "        sf.write(out_path, y_norm, sr) \n",
    "        normalized_paths.append(out_path)\n",
    "    return normalized_paths\n",
    "\n",
    "# Apply to all splits\n",
    "train_norm_files = normalize_and_save(train_df[\"filepath\"], mean, std, f\"{output_dir}/train\")\n",
    "val_norm_files   = normalize_and_save(val_df[\"filepath\"], mean, std, f\"{output_dir}/val\")\n",
    "test_norm_files  = normalize_and_save(test_df[\"filepath\"], mean, std, f\"{output_dir}/test\")\n",
    "\n",
    "print(f\"✅ Normalized {len(train_norm_files)} train, {len(val_norm_files)} val, {len(test_norm_files)} test files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665991f0",
   "metadata": {},
   "source": [
    "Section 4: Feature Extraction (MFCC)\n",
    "\n",
    "Extract MFCCs from the normalized audio files.\n",
    "MFCCs describe the spectral envelope and frequency dynamics of baby cries,\n",
    "serving as the foundation for both baseline and deep learning models.\n",
    "- For the baseline SVM model, we will later summarize each MFCC sequence into a fixed-length vector (mean + std).\n",
    "- For the CNN model, we will keep the full MFCC spectrograms as 2D inputs.\n",
    "\n",
    "Output files:\n",
    "- data/mfcc/train_mfcc.npy\n",
    "- data/mfcc/val_mfcc.npy\n",
    "- data/mfcc/test_mfcc.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b65626e-79db-433f-9103-126152bfe9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Define parameters ---\n",
    "N_MFCC = 40        # number of MFCC coefficients to extract\n",
    "SR = 16000         # sampling rate (same as preprocessing)\n",
    "DURATION = 8.0     # fixed duration in seconds\n",
    "SAMPLES = int(SR * DURATION)\n",
    "\n",
    "def extract_mfcc(file_path, n_mfcc=N_MFCC, sr=SR):\n",
    "    \"\"\"\n",
    "    Extract MFCCs from one normalized audio file.\n",
    "    Returns: MFCC array of shape [n_mfcc, time_frames].\n",
    "    \"\"\"\n",
    "    y, _ = librosa.load(file_path, sr=sr, mono=True)\n",
    "    # pad/truncate to fixed length for consistency\n",
    "    if len(y) < SAMPLES:\n",
    "        y = np.pad(y, (0, SAMPLES - len(y)))\n",
    "    else:\n",
    "        y = y[:SAMPLES]\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfcc\n",
    "\n",
    "# --- Helper function to process a split ---\n",
    "def process_split(split_name, folder_path, n_mfcc=N_MFCC):\n",
    "    print(f\"Extracting MFCCs for {split_name} split...\")\n",
    "    filepaths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
    "    X = []\n",
    "    for f in tqdm(filepaths):\n",
    "        mfcc = extract_mfcc(f, n_mfcc=n_mfcc)\n",
    "        X.append(mfcc)\n",
    "    os.makedirs(\"data/mfcc\", exist_ok=True)\n",
    "    \n",
    "    X = np.array(X, dtype=object)\n",
    "    np.save(f\"data/mfcc/{split_name}_mfcc.npy\", X)\n",
    "    print(f\"✅ Saved {split_name} MFCCs to data/mfcc/{split_name}_mfcc.npy\")\n",
    "    return X\n",
    "\n",
    "# --- Run for all splits ---\n",
    "train_mfcc = process_split(\"train\", \"data/normalized_wav/train\")\n",
    "val_mfcc   = process_split(\"val\", \"data/normalized_wav/val\")\n",
    "test_mfcc  = process_split(\"test\", \"data/normalized_wav/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f044127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mfcc.npy files\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_mfcc = np.load(\"data/mfcc/train_mfcc.npy\", allow_pickle=True)\n",
    "val_mfcc   = np.load(\"data/mfcc/val_mfcc.npy\", allow_pickle=True)\n",
    "test_mfcc  = np.load(\"data/mfcc/test_mfcc.npy\", allow_pickle=True)\n",
    "\n",
    "print(f\"Train MFCC count: {len(train_mfcc)}\")\n",
    "print(f\"One sample shape: {train_mfcc[0].shape}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mfcc_sample = np.array(train_mfcc[0], dtype=float)\n",
    "plt.imshow(mfcc_sample, aspect='auto', origin='lower')\n",
    "plt.title(\"MFCC Example\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b75c3",
   "metadata": {},
   "source": [
    "Secion 4.1: Use Modular MFCC Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b66ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import extract_split_mfcc\n",
    "\n",
    "train_mfcc = extract_split_mfcc(\"train\", \"data/normalized_wav/train\")\n",
    "val_mfcc   = extract_split_mfcc(\"val\", \"data/normalized_wav/val\")\n",
    "test_mfcc  = extract_split_mfcc(\"test\", \"data/normalized_wav/test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
